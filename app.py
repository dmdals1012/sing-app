import streamlit as st
import numpy as np
import librosa
import joblib
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
import librosa.display
import av
import queue
import time
import asyncio
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration

# ë””ë²„ê¹… ì¶œë ¥
st.write("ğŸ”¹ Streamlit WebRTC ì•± ì‹œì‘")

# ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_model():
    st.write("âœ… ëª¨ë¸ ë¡œë“œ ì¤‘...")
    return keras.models.load_model('vocal_range_classifier.h5')

try:
    model = load_model()
    st.write("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ
try:
    label_encoder = joblib.load('label_encoder.pkl')
    st.write("âœ… ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    st.error(f"âŒ ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ ì‹¤íŒ¨: {e}")

st.title('ì‹¤ì‹œê°„ ìŒì—­ëŒ€ ë¶„ë¥˜ê¸°')

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'audio_data' not in st.session_state:
    st.session_state.audio_data = None
if 'recording_state' not in st.session_state:
    st.session_state.recording_state = 'stopped'

audio_buffer = asyncio.Queue()

# ì˜¤ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬
async def audio_frame_callback(frame):
    if st.session_state.recording_state == 'recording':
        sound = frame.to_ndarray().flatten().astype(np.float32) / 32768.0
        await audio_buffer.put(sound)
    return frame

# ë¹„ë™ê¸° ë…¹ìŒ í•¨ìˆ˜
async def record_audio():
    st.write("ë…¹ìŒ ì¤‘... 5ì´ˆ ë™ì•ˆ ë…¸ë˜ë¥¼ ë¶€ë¥´ì„¸ìš”.")
    start_time = time.time()
    audio_frames = []
    
    while time.time() - start_time < 5:
        try:
            audio_frame = await asyncio.wait_for(audio_buffer.get(), timeout=0.1)
            audio_frames.append(audio_frame)
        except asyncio.TimeoutError:
            continue
    
    if audio_frames:
        st.session_state.audio_data = np.concatenate(audio_frames, axis=0)
    else:
        st.error("ë…¹ìŒëœ ì˜¤ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ ê¶Œí•œì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    st.session_state.recording_state = 'stopped'

# WebRTC ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (ì„¤ì • ë³€ê²½)
try:
    webrtc_ctx = webrtc_streamer(
        key="audio-recorder",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTCConfiguration(
            {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
        ),
        media_stream_constraints={"video": False, "audio": True},
        async_processing=True,
        audio_frame_callback=audio_frame_callback
    )
    st.write("âœ… WebRTC ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
except Exception as e:
    st.error(f"âŒ WebRTC ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

if webrtc_ctx.state.playing:
    if st.session_state.recording_state == 'stopped':
        st.session_state.recording_state = 'recording'
        asyncio.run(record_audio())

if st.session_state.audio_data is not None:
    audio_data = st.session_state.audio_data
    
    def extract_features(audio, sr):
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)
        return np.mean(mfcc.T, axis=0)

    features = extract_features(audio_data, sr=48000)
    features = features.reshape(1, -1)
    
    try:
        prediction = model.predict(features)
        predicted_label = np.argmax(prediction, axis=1)
        voice_type = label_encoder.inverse_transform(predicted_label)[0]
        st.write(f"ì˜ˆì¸¡ëœ ìŒì—­ëŒ€: {voice_type}")
    except Exception as e:
        st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    st.write("MFCC ìŠ¤í™íŠ¸ë¡œê·¸ë¨:")
    fig, ax = plt.subplots()
    mfcc = librosa.feature.mfcc(y=audio_data, sr=48000, n_mfcc=40)
    librosa.display.specshow(mfcc, sr=48000, x_axis='time', ax=ax)
    plt.colorbar()
    st.pyplot(fig)
    
    st.session_state.audio_data = None

st.write("ì‚¬ìš© ë°©ë²•: 'START' ë²„íŠ¼ì„ í´ë¦­í•˜ê³  5ì´ˆ ë™ì•ˆ ë…¸ë˜ë¥¼ ë¶€ë¥´ì„¸ìš”.")
