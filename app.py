import streamlit as st
import numpy as np
import librosa
import joblib
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
import librosa.display
import av
import queue
import time
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration

# ë””ë²„ê¹… ì¶œë ¥
st.write("ğŸ”¹ Streamlit WebRTC ì•± ì‹œì‘")

# ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_model():
    st.write("âœ… ëª¨ë¸ ë¡œë“œ ì¤‘...")
    return keras.models.load_model('vocal_range_classifier.h5')

try:
    model = load_model()
    st.write("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ
try:
    label_encoder = joblib.load('label_encoder.pkl')
    st.write("âœ… ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    st.error(f"âŒ ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ ì‹¤íŒ¨: {e}")

st.title('ì‹¤ì‹œê°„ ìŒì—­ëŒ€ ë¶„ë¥˜ê¸°')

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'audio_data' not in st.session_state:
    st.session_state.audio_data = None
if 'recording_state' not in st.session_state:
    st.session_state.recording_state = 'stopped'

audio_buffer = queue.Queue()

# ì˜¤ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬
def audio_frame_callback(frame):
    if st.session_state.recording_state == 'recording':
        sound = frame.to_ndarray().flatten().astype(np.float32) / 32768.0
        audio_buffer.put(sound)
    return frame

# WebRTC ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (ì„¤ì • ë³€ê²½)
try:
    webrtc_ctx = webrtc_streamer(
        key="audio-recorder",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration=RTCConfiguration(
            {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
        ),
        media_stream_constraints={"video": False, "audio": True},
        audio_frame_callback=audio_frame_callback
    )
    st.write("âœ… WebRTC ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
except Exception as e:
    st.error(f"âŒ WebRTC ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

if webrtc_ctx.state.playing:
    if st.session_state.recording_state == 'stopped':
        st.session_state.recording_state = 'recording'
        st.write("ë…¹ìŒ ì¤‘... 5ì´ˆ ë™ì•ˆ ë…¸ë˜ë¥¼ ë¶€ë¥´ì„¸ìš”.")
        start_time = time.time()
        audio_frames = []
        
        while time.time() - start_time < 5:
            try:
                audio_frame = audio_buffer.get(timeout=0.1)
                audio_frames.append(audio_frame)
            except queue.Empty:
                continue
        
        st.session_state.recording_state = 'stopped'
        
        if audio_frames:
            st.session_state.audio_data = np.concatenate(audio_frames, axis=0)
        else:
            st.error("ë…¹ìŒëœ ì˜¤ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ ê¶Œí•œì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if st.session_state.audio_data is not None:
    audio_data = st.session_state.audio_data
    
    def extract_features(audio, sr):
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)
        return np.mean(mfcc.T, axis=0)

    features = extract_features(audio_data, sr=48000)
    features = features.reshape(1, -1)
    
    try:
        prediction = model.predict(features)
        predicted_label = np.argmax(prediction, axis=1)
        voice_type = label_encoder.inverse_transform(predicted_label)[0]
        st.write(f"ì˜ˆì¸¡ëœ ìŒì—­ëŒ€: {voice_type}")
    except Exception as e:
        st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    st.write("MFCC ìŠ¤í™íŠ¸ë¡œê·¸ë¨:")
    fig, ax = plt.subplots()
    mfcc = librosa.feature.mfcc(y=audio_data, sr=48000, n_mfcc=40)
    librosa.display.specshow(mfcc, sr=48000, x_axis='time', ax=ax)
    plt.colorbar()
    st.pyplot(fig)
    
    st.session_state.audio_data = None

st.write("ì‚¬ìš© ë°©ë²•: 'START' ë²„íŠ¼ì„ í´ë¦­í•˜ê³  5ì´ˆ ë™ì•ˆ ë…¸ë˜ë¥¼ ë¶€ë¥´ì„¸ìš”.")
